{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Turnip_CNN_GitHub.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNS/p5gzem9LVJpmMIjtE2b"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoZUN1LnL74h"
      },
      "source": [
        "#import necessary packages\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
        "\n",
        "from keras import models\n",
        "\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "# starting point \n",
        "my_model= models.Sequential()\n",
        "\n",
        "# Add first convolutional block\n",
        "my_model.add(Conv2D(16, (3, 3), activation='relu', padding='same', \n",
        "                    input_shape=(320,240,3)))\n",
        "my_model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "\n",
        "# second block\n",
        "my_model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "my_model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "# third block\n",
        "my_model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "my_model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "# fourth block\n",
        "my_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
        "my_model.add(MaxPooling2D((2, 2), padding='same'))\n",
        "\n",
        "# global average pooling\n",
        "my_model.add(GlobalAveragePooling2D())\n",
        "# fully connected layer\n",
        "my_model.add(Dense(64, activation='relu'))\n",
        "my_model.add(BatchNormalization())\n",
        "# make predictions\n",
        "my_model.add(Dense(2, activation='sigmoid'))\n",
        "\n",
        "\n",
        "# Show a summary of the model. Check the number of trainable parameters\n",
        "my_model.summary()\n",
        "\n",
        "# use early stopping to optimally terminate training through callbacks\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "\n",
        "# save best model automatically\n",
        "mc= ModelCheckpoint('leafdata/your_model.h5', monitor='val_loss',mode='min', verbose=1, save_best_only=True)\n",
        "cb_list=[es,mc]\n",
        "\n",
        "\n",
        "# compile model \n",
        "my_model.compile(optimizer='adam', loss='binary_crossentropy', \n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "\n",
        "from tensorflow.python.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# set up data generator\n",
        "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "# get training image batches from the directory\n",
        "train_generator = data_generator.flow_from_directory(\n",
        "        'leafdata/Training',\n",
        "        target_size=(178, 218),\n",
        "        batch_size=12, shuffle = \"True\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "# get validation image batches from the directory\n",
        "validation_generator = data_generator.flow_from_directory(\n",
        "        'leafdata/Validation',\n",
        "        target_size=(178, 218),\n",
        "        batch_size=12, shuffle = \"True\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "#tensorboard visualizations\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "#fit model\n",
        "history = my_model.fit(\n",
        "        train_generator,\n",
        "        epochs=30,\n",
        "        steps_per_epoch=3,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=3, callbacks=[cb_list, tensorboard_callback])\n",
        "\n",
        "#evaluate best model\n",
        "from keras.models import load_model\n",
        "saved_model = load_model('leafdata/your_model.h5')\n",
        "test_generator = data_generator.flow_from_directory(\n",
        "        'leafdata/Testing',\n",
        "        target_size=(178, 218),\n",
        "        batch_size=12,\n",
        "        class_mode='categorical')\n",
        "saved_model.evaluate(test_generator)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}